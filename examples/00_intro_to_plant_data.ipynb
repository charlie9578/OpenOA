{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db2be98-8ff9-4e24-9c84-937416d3f4df",
   "metadata": {},
   "source": [
    "#ds. For the OpenOA examples, the metadatafile: `data/plant_meta.yml` will be used (and a JSON reference for those that prefer JSON: `data/plant_meta.json`) to map the La Haute Borne fields to the OpenOA fields. This v3 update allows a user to bring their data directly into a `PlantData` object with a means for the OpenOA to know which data fields are being used \n",
    "\n",
    "Below is a demonstration of loading a `PlantMetaData`object directly to show what data are\n",
    "expected, though there is a `PlantMetaData.load()` that can accept a dictionary or file path input for routinized workflows.\n",
    "\n",
    "```python\n",
    "metadata = PlantMetaData(\n",
    "    latitude,  # float\n",
    "    longitude,  # float\n",
    "    scada,  # dictionary of column mappings and data frequency\n",
    "    meter,  # dictionary of column mappings and data frequency\n",
    "    tower,  # dictionary of column mappings and data frequency\n",
    "    status,  # dictionary of column mappings and data frequency\n",
    "    curtail,  # dictionary of column mappings and data frequency\n",
    "    asset,  # dictionary of column mappings\n",
    "    reanalysis,  # dictionary of each product's dictionary of column mappings and data frequency\n",
    ")\n",
    "```\n",
    "For each of the data objects above, there is a corresponding meta data class to help guide users. For instance, the `SCADAMetaData` (below) has pre-set attributes to help guide users outside of the docstrings and standard documentation. The other meta data objects are: `MeterMetaData`, `TowerMetaData`, `StatusMetaData`, `CurtailMetaData`, `AssetMetaData`, and `ReanalysisMetaData` (one is created for each producted provided).\n",
    "\n",
    "For example, each of the metadata classes allows inputs for the column mappings and timestamp frequency to enable the data validation steps outlined in the [process summary](#summarizing-the-qa-process-into-a-reproducible-workflow). However to clarify the units and data types expected, each of the metadata classes contains the immutable attributes: `units` and `dtypes`, as shown below for the `SCADAMetaData` class, to signal to users what units each data input should be in, when passed, and what type the data should be able to be converted to, if it's not already in that format. Some examples of acceptable formats would be string-encode floats, or string-encoded timestamps, both of which can be automatically converted in the initialization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c103511c-f573-41d4-8e01-f93a056f1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected units for each column in the SCADA data:\n",
      "{'WMET_EnvTmp': 'C',\n",
      " 'WMET_HorWdDir': 'deg',\n",
      " 'WMET_HorWdDirRel': 'deg',\n",
      " 'WMET_HorWdSpd': 'm/s',\n",
      " 'WROT_BlPthAngVal': 'deg',\n",
      " 'WTUR_SupWh': 'kWh',\n",
      " 'WTUR_TurSt': None,\n",
      " 'WTUR_W': 'kW',\n",
      " 'asset_id': None,\n",
      " 'time': 'datetim64[ns]'}\n",
      "\n",
      "Expected data types for each column in the SCADA data:\n",
      "{'WMET_EnvTmp': <class 'float'>,\n",
      " 'WMET_HorWdDir': <class 'float'>,\n",
      " 'WMET_HorWdDirRel': <class 'float'>,\n",
      " 'WMET_HorWdSpd': <class 'float'>,\n",
      " 'WROT_BlPthAngVal': <class 'float'>,\n",
      " 'WTUR_SupWh': <class 'float'>,\n",
      " 'WTUR_TurSt': <class 'str'>,\n",
      " 'WTUR_W': <class 'float'>,\n",
      " 'asset_id': <class 'str'>,\n",
      " 'time': <class 'numpy.datetime64'>}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from openoa.schema import SCADAMetaData\n",
    "\n",
    "scada_meta = SCADAMetaData()  # no inputs means use the default, internal mappings\n",
    "print(\"Expected units for each column in the SCADA data:\")\n",
    "pprint(scada_meta.units)\n",
    "print()\n",
    "print(\"Expected data types for each column in the SCADA data:\")\n",
    "pprint(scada_meta.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03a142-6321-4b3c-af00-ffa8a221fa29",
   "metadata": {},
   "source": [
    "Below is a demonstration of loading a `PlantData` object directly, though there are class methods for loading from file or an ENTR warehouse.\n",
    "```python\n",
    "plant = PlantData(\n",
    "    metadata,  # PlantMetaData, dictionary, or file\n",
    "    analysis_type,  # list of analysis types expected to be performed, \"all\", or None\n",
    "    scada,  # None, DataFrame or CSV file path\n",
    "    meter,  # None, DataFrame or CSV file path\n",
    "    tower,  # None, DataFrame or CSV file path\n",
    "    status,  # None, DataFrame or CSV file path\n",
    "    curtail,  # None, DataFrame or CSV file path\n",
    "    asset,  # None, DataFrame or CSV file path\n",
    "    reanalysis,  # None, dictionary of DataFrames or CSV file paths with the name of the product for keys\n",
    ")\n",
    "```\n",
    "\n",
    "On loading, the data will be validated automatically according to the `analysis_type` input(s) provided to ensure columns exist with the expected names, data types are correct, and data frequencies are of a sufficient resolution. However, while all erros in this process are caught, only those of concern to an `analysis_type` are raised, with the exception of \"all\" raises any error found and `None` ignore all errors.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87ca084-5f82-452e-be7d-48256b786519",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5pyd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenoa\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PlantData\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenoa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m qa\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenoa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mproject_ENGIE\u001b[39;00m\n",
      "File \u001b[0;32m/layers/paketo-buildpacks_pip-install/packages/lib/python3.10/site-packages/openoa/utils/qa.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytz\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mh5pyd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5pyd'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from openoa import PlantData\n",
    "from openoa.utils import qa\n",
    "from openoa.utils import plot\n",
    "\n",
    "import project_ENGIE\n",
    "\n",
    "# Avoid clipping data previews unnecessarily\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660921a-0a84-450d-b237-12580beb186e",
   "metadata": {},
   "source": [
    "## QA'ing ENGIE's open data set\n",
    "\n",
    "ENGIE provides access to the data of its 'La Haute Borne' wind farm through https://opendata-renewables.engie.com and through an API. The data gives users the opportunity to work with real-world operational data. \n",
    "\n",
    "The series of notebooks in the 'examples' folder uses SCADA data downloaded from https://opendata-renewables.engie.com, saved in the `examples/data` folder. Additional plant level meter, availability, and curtailment data were synthesized based on the SCADA data.\n",
    "\n",
    "The data used throughout these examples are pre-processed appropriately for the issues described in the subsequent sections, and synthesized into a routinized format in the `examples/project_ENGIE.py` Python script.\n",
    "\n",
    "**Note**: This demonstration is centered around a specific data set, so it should be noted that there are other methods for working with data that are not featured here, and we would like to point the user to the API documentation for further data checking and manipulation methods.\n",
    "\n",
    "### Step 1: Load the SCADA data\n",
    "\n",
    "First we'll need to unzip the data, and read the SCADA data to a pandas `DataFrame` so we can take a look at the data before we can start working with it. Here the `project_ENGIE.extract_data()` method is used to unzip the data folder because this demonstration is based on working with the ENGIE provided data without any preprocessing steps taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c23a6-77bb-47ea-ab39-0375df436f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/la_haute_borne\"\n",
    "project_ENGIE.extract_data(data_path)\n",
    "\n",
    "scada_df = pd.read_csv(f\"{data_path}/la-haute-borne-data-2014-2015.csv\")\n",
    "\n",
    "scada_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fec6bc-5e8d-4ae8-828d-a2bc810ed277",
   "metadata": {},
   "source": [
    "The timestamps in the column `Date_time` show that we have timezone information encoded, and that the data have a 10 minute frequency to them (or \"10min\" according to the pandas guidance: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases)\n",
    "\n",
    "To demonstrate the breadth of data that the QA methods are inteneded to handle this demonstration will step through the data using the current format, and an alternative where the timezone data has been stripped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8cc74-4262-4e64-972c-923773e6256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_df_tz = scada_df.loc[:, :].copy()  # timezone aware\n",
    "scada_df_no_tz = scada_df.loc[:, :].copy()  # timezone unaware\n",
    "\n",
    "# Remove the timezone information from the timezone unaware example dataframe\n",
    "scada_df_no_tz.Date_time = scada_df_no_tz.Date_time.str[:19].str.replace(\"T\", \" \")\n",
    "\n",
    "# # Show the resulting change\n",
    "scada_df_no_tz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8a219-9194-47bd-b416-3da6c689b1ca",
   "metadata": {},
   "source": [
    "Below, we can see the data types for each of the columns. We should note that the timestamps are not correctly encoded, but are considered as objects at this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce79a2-24f5-4259-909a-2aa42a583501",
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_df_tz.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a485866-7138-4634-b12d-853ec1f387f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_df_no_tz.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73013248-a28e-448d-bec2-a543fe374080",
   "metadata": {},
   "source": [
    "### Step 2: Convert the timestamps to proper timestamp data objects\n",
    "\n",
    "Using the `qa.convert_datetime_column()` method, we can convert the timestamp data accordingly and insert the UTC-encoded data as an index for both the timezone aware, and timezone unaware data sets.\n",
    "\n",
    "Under the hood this method does a few helpful items to create the resulting data set:\n",
    "1) Converts the column \"Date_time\" to a datetime object\n",
    "2) Creates the new datetime columns: \"Date_time_localized\" and \"Date_time_utc\" for the localized and UTC-encoded datetime objects\n",
    "3) Sets the UTC timestamp as the index\n",
    "4) Creates the column \"utc_offset\" containing the difference between the UTC timestamp and the localized timestamp that will be used to determine if the timestamp is in DST or not.\n",
    "5) Creates the column \"is_dst\" indicating if the timestamps are in DST (`True`), or not (`False`) that will be used later when trying to assess time gaps and duplications in the data\n",
    "\n",
    "Notice that in the resulting data that the data type of the column \"Date_time\" is successfully made into a localized timestamp in the timezone aware example, but is kept as a non-localized timestamp in the unaware example.\n",
    "\n",
    "In the below, the \"Date_time_utc\" column should always remain in UTC time and the \"Date_time_localized\" column should always remain in the localized time. Conveniently, Pandas provides two methods `tz_convert()` and `tz_localize()` to toggle back and forth between timezones, which will operate on the index of the DataFrame. It is worth noting that the local time could also be UTC, in which case the two columns would be redundant.\n",
    "\n",
    "The localized time, even when the passed data is unaware, is adjusted using the `local_tz` keyword argument to help normalize the time strings, from which a UTC-based timestamp is created (even when local is also UTC). By calculating the UTC time from the local time, we are able to ascertain DST shifts in the data, and better assess any anomalies that may exist.\n",
    "\n",
    "However, there may be cases where the timezone is neither encoded (the unaware example), nor known. In the former, we can use the `local_tz` keyword argument that is seen in the code above, but for the latter, this is much more difficult, and the default value of UTC may not be accurate. In this latter case it is useful to try multiple timezones, such as an operating/owner company's headquarters or often the windfarm's location to find a best fit. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "In the case of US-based wind power plants, the \"qa.wtk_xx()\" methods, such as \"qa.wtk_diurnal_prep()\" and \"qa.wtk_diurnal_plot()\", can be used for working with NREL's WINDToolKit for further data checking, validation, and plotting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a340d-7289-4f62-a8ce-4ca81173e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_df_tz = qa.convert_datetime_column(\n",
    "    df=scada_df_tz,\n",
    "    time_col=\"Date_time\",\n",
    "    local_tz=\"Europe/Paris\",\n",
    "    tz_aware=True # Indicate that we can use encoded data to convert between timezones\n",
    ")\n",
    "scada_df_tz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1d8ef-9378-415a-801d-cc4d12d9e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scada_df_tz.index.dtype)\n",
    "scada_df_tz.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cf634-e748-4826-8de2-1531d06dadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_df_no_tz = qa.convert_datetime_column(\n",
    "    df=scada_df_no_tz,\n",
    "    time_col=\"Date_time\",\n",
    "    local_tz=\"Europe/Paris\",\n",
    "    tz_aware=False  # Indicates that we're going to need to make inferences about encoding the timezones\n",
    ")\n",
    "scada_df_no_tz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abba33-54f9-4824-b67c-1e9ecb7cc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scada_df_no_tz.index.dtype)\n",
    "scada_df_no_tz.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524815c-30c3-4f7a-88ec-ffcacce5ca59",
   "metadata": {},
   "source": [
    "### Step 3: Dive into the data\n",
    "\n",
    "Using the `describe` method, which is a thin wrapper for the pandas method shows us the distribution of each of the numeric and time-based data columns. Notice that both descriptions are equal, with the exception of the UTC offset, because they are the same data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650eb4e0-200f-4e0b-ace7-d22850e07ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tz = qa.describe(scada_df_no_tz)\n",
    "no_tz = no_tz.loc[~no_tz.index.isin([\"Date_time\"])] # Ignore the Date_time column that is not shared between the dataframes\n",
    "col_order = [\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"] # Ensure description columns are in the same order\n",
    "qa.describe(scada_df_tz)[col_order] == no_tz[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb50f8c-8ac3-436f-8dec-bc3c6c779a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.describe(scada_df_tz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0217e2-c2c9-4a6c-9b6f-0a7697b7675e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inspecting the distributions of each column of numerical data\n",
    "\n",
    "Similar to the above, `column_histograms` is not part of the QA module, but is helpful for reviewing the independent distributions of data within a dataset. Aligning with the table version below, we can see that some distrbiutions, such as \"Ws_avg\" don't have any outliers, whereas others such as \"Ot_avg\" do and have very narrow histograms to accommadate this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c058769-d3a6-497b-9d67-119379f53c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.column_histograms(scada_df_tz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678dc0f-3d9a-4183-b588-d0dcf29097ba",
   "metadata": {},
   "source": [
    "It appears that there are a number of highly frequent values in these distributions, so we can dive into that further to see if we have some unresponsive sensors, in which case the data will need to be invalidated for later analysis. In the below analysis of repeated behaviors in the data, it seems that we should be flagging potentially unresponsive sensors (see Step 5 for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f188097-4753-4d18-a6b3-73c9f4f2a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only check data for a single turbine to avoid any spurious findings\n",
    "single_turbine_df = scada_df_tz.loc[scada_df_tz.Wind_turbine_name == \"R80736\"].copy()\n",
    "\n",
    "# Identify consecutive data readings\n",
    "ix_consecutive = single_turbine_df.Va_avg.diff(1) != 0\n",
    "\n",
    "# Determine how many consecutive occurences are for various thresholds, starting with 2 repeats\n",
    "consecutive_counts = {i + 1: (ix_consecutive.rolling(i).sum() == 0).sum() for i in range(1, 10)}\n",
    "\n",
    "# Plot the distribution of  N occurences for each threshold\n",
    "plt.bar(consecutive_counts.keys(), consecutive_counts.values(), zorder=10)\n",
    "plt.grid(zorder=0)\n",
    "plt.xticks(range(2, 10))\n",
    "plt.xlim((1, 10))\n",
    "plt.ylim(0, 100)\n",
    "plt.ylabel(\"Number of Consecutive Repeats\")\n",
    "plt.xlabel(\"Threshold for Consecutive Repeats\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8cd46-1434-498a-a5b1-52b3d9bd5a42",
   "metadata": {},
   "source": [
    "It's evident in the above distribution that this sensor appears to be operating adequately and won't need to have any data flagged for unresponsiveness. However, in the below example, we can see that the temperature data are potentially having faulty data and should therefore be flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7db3d-7162-47d8-8edc-af16641f51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify consecutive data readings\n",
    "ix_consecutive = single_turbine_df.Ot_avg.diff(1) != 0\n",
    "\n",
    "# Determine how many consecutive occurences are for various thresholds, starting with 2 repeats\n",
    "consecutive_counts = {i + 1: (ix_consecutive.rolling(i).sum() == 0).sum() for i in range(1, 10)}\n",
    "\n",
    "# Plot the distribution of  N occurences for each threshold\n",
    "plt.bar(consecutive_counts.keys(), consecutive_counts.values(), zorder=10)\n",
    "plt.grid(zorder=0)\n",
    "plt.xlim((1, 10))\n",
    "plt.ylim(0, 5500)\n",
    "plt.xticks(range(2, 10))\n",
    "plt.yticks(range(0, 5501, 500))\n",
    "plt.ylabel(\"Number of Consecutive Repeats\")\n",
    "plt.xlabel(\"Threshold for Consecutive Repeats\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6491a-d8d2-489a-aa69-cb5c501d44db",
   "metadata": {},
   "source": [
    "#### Checking the power curve distributions\n",
    "\n",
    "While not contained in the QA module, the `plot_by_id` method is helpful for quickly assessing the quality of our operational power curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36f8b9-78ff-4491-9441-337f4dce7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_by_id(\n",
    "    df=scada_df_no_tz,\n",
    "    id_col=\"Wind_turbine_name\",\n",
    "    x_axis=\"Ws_avg\",\n",
    "    y_axis=\"P_avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa2919-a520-47ba-8097-4fac8443b7b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Inspecting the timestamps for DST gaps and duplications\n",
    "\n",
    "Now, we can get the the duplicate time stamps from each of the data sets, according to each of the original, localized, and UTC time data. This will help us to compare the effects of DST and timezone encoding.\n",
    "\n",
    "In the below, timezone unaware data, we can see that there is a significant deviation between the local timestamps and the UTC timestamps, especialy around the end of March in both 2018 and 2019, suggesting that there is something missing with the DST data. \n",
    "\n",
    "#### Timezone-unaware data\n",
    "\n",
    "First we'll look only at the data without any timezone encoding, then compare the results to the data where we kept the timezone data encoded to confirm what modifications need to be made to the data. Notice that the UTC converted data are showing duplications at roughly the same time each year when the spring-time European DST shift occurs, and is likely indicating that the original datetime stamps are missing the data to properly shift the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fabae1b-ac1b-4f2e-839d-30a5fece0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_orig_no_tz, dup_local_no_tz, dup_utc_no_tz = qa.duplicate_time_identification(\n",
    "    df=scada_df_no_tz,\n",
    "    time_col=\"Date_time\",\n",
    "    id_col=\"Wind_turbine_name\"\n",
    ")\n",
    "dup_orig_no_tz.size, dup_local_no_tz.size, dup_utc_no_tz.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3d049-1def-4801-a448-95352beaa17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dup_utc_no_tz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b629e-9568-449c-b2ef-3088cab03c4d",
   "metadata": {},
   "source": [
    "To help confirm there are DST corrections needed in the data, we can also take a lot at the gaps in the timestamps, particularly in October. At a quick glance, the timezone unaware UTC encoding seems to create gaps in the data, likely accounting for the DST shift in the fall.\n",
    "\n",
    "Based on the duplicated timestamps in the original data, it does seem like there is a DST correction in spring but no duplicate times in the fall. However, even with a UTC conversion, there still appear to be duplications in the data, so there is likely additional analysis needed here. While it appears that there are time gaps in the data for the original inputs, this phenomena switches seasons to the fall for the UTC converted time stamps, likely due to the lack of timezone encoding in the original inputs compared to a corrected timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c08a11-162f-42ee-9d96-ddad4692fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_orig_no_tz, gap_local_no_tz, gap_utc_no_tz = qa.gap_time_identification(\n",
    "    df=scada_df_no_tz,\n",
    "    time_col=\"Date_time\",\n",
    "    freq=\"10min\"\n",
    ")\n",
    "gap_orig_no_tz.size, gap_local_no_tz.size, gap_utc_no_tz.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0b316-1075-45c8-bbd5-304bb6f99e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_orig_no_tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1608a2-d07a-4ddf-bda4-b9a201819a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_utc_no_tz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e2f61-bdb0-4d0e-8dfb-3bbab7b1dd73",
   "metadata": {},
   "source": [
    "Below, we can observe the effects of not having timezones encoded, and what that might mean for potential analyses. In the unaware data, it appears that the original data (blue, solid line, labeled \"Original Timestamp\") has a time gap in the spring; however, when we compare it to the UTC timestamp (orange, dashed line), it is clear that there is not in fact any gap in the data, and the DST transition has been encoded properly in the data. On the otherhand, it at first appears that there are no gaps in the fall when we make the same comparison, but when looking at the UTC timestamps, we can see that there is a 1 hour gap in the data for both 2014 and 2015. This is in line with our comparison of the original and UTC time gaps above, and further confirms our findings that there are duplicates in the spring and gaps in the fall.\n",
    "\n",
    "By having the original data and a UTC-converted timestamp it enables us to see any gaps that may appear when there is no timezone data encoded. On the other hand, using the UTC-converted timestamp does not reduce the number of duplications in this dataset that are present in the spring, but helps adjust for seemingly missing or available data. In tandem we can see in the scatter points that there are still duplicates in the spring data just before the DST switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54900a31-1453-4483-83c7-507270bf428e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Timezone Unaware\n",
    "qa.daylight_savings_plot(\n",
    "    df=scada_df_no_tz,\n",
    "    local_tz=\"Europe/Paris\",\n",
    "    id_col=\"Wind_turbine_name\",\n",
    "    time_col=\"Date_time\",\n",
    "    power_col=\"P_avg\",\n",
    "    freq=\"10min\",\n",
    "    hour_window=3  # default value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e6157-0453-4657-91f8-8bd69bc06252",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Timezone-aware data\n",
    "\n",
    "We see a similar finding for timezeone-aware data, below, for the both the number of duplications and gaps, likely confirming our hunches from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ff1e6-e6c4-4a51-8b13-5c4b980a5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_orig_tz, dup_local_tz, dup_utc_tz = qa.duplicate_time_identification(\n",
    "    df=scada_df_tz,\n",
    "    time_col=\"Date_time\",\n",
    "    id_col=\"Wind_turbine_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84179087-b025-4c64-ba5b-b77d4c3ed7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_orig_tz.size, dup_local_tz.size, dup_utc_tz.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f78d4e-0752-4aed-8b43-5aba9f3807b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_utc_tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f745aa1-cecc-485f-bd69-b0699512f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_orig_tz, gap_local_tz, gap_utc_tz = qa.gap_time_identification(\n",
    "    df=scada_df_tz,\n",
    "    time_col=\"Date_time\",\n",
    "    freq=\"10min\"\n",
    ")\n",
    "gap_orig_tz.size, gap_local_tz.size, gap_utc_tz.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa6487-91b1-473e-98db-1e510a0be0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_utc_tz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bbfb12-0408-4954-a133-0df845a96354",
   "metadata": {},
   "source": [
    "Again, we see a high degree of similarity between the two examples, and so can confirm that we have some duplicated data in the spring unrelated to the DST shift, and some missing data in the fall likely due to the DST shift. Additionally, we can confirm that the Europe/Paris timezone is in fact the encoding of our original data, and should therefore be converted to UTC for later analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27f3d9-f012-480d-9561-185fa191da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timezone Aware\n",
    "qa.daylight_savings_plot(\n",
    "    df=scada_df_tz,\n",
    "    local_tz=\"Europe/Paris\",\n",
    "    id_col=\"Wind_turbine_name\",\n",
    "    time_col=\"Date_time\",\n",
    "    power_col=\"P_avg\",\n",
    "    freq=\"10min\",\n",
    "    hour_window=3  # default value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93768d-59be-4798-9c0f-0d0461746252",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarizing the QA process into a reproducible workflow\n",
    "\n",
    "The following description summarizes the steps taken to successfully import the ENGIE SCADA data based on the above analysis, and are implemented in the `project_ENGIE.prepare()` method. It should be noted that this method is cleaned up to provide users with an easy to follow example, it could also be contained in an analysis notebook, stand-alone script, etc., as long as it is able to feed into `PlantData` at the end of it.\n",
    "\n",
    "1. From [Step 2](#step-2-convert-the-timestamps-to-proper-timestamp-data-objects) and [Step 4](#step-4-inspecting-the-timestamps-for-dst-gaps-and-duplications) we found that the data is in local time and should be converted to UTC for clarity in the timestamps. THis corresponds with line `project_ENGIE.py:79`.\n",
    "2. Additionally from [Step 4](#step-4-inspecting-the-timestamps-for-dst-gaps-and-duplications), it was clear that duplicated timestamp data will need to be removed, corresponding to line `project_ENGIE.py:82`\n",
    "3. In [Step 3](#step-3-dive-into-the-data), there is an oversized range for the temperature data, so this data will be invalidated, corresponding to line `project_ENGIE.py:86`\n",
    "4. In [Step 3](#step-3-dive-into-the-data), the wind vane direction (\"Va_avg\") and temperature (\"Ot_avg\") fields seemed to have a large number duplicated data that were identified, so these data are flagged and invalidated, which corresponds to lines `project_ENGIE.py:88-102`\n",
    "5. Finally, in [Step 3](#step-3-dive-into-the-data), it also should be noted that the pitch direction ranges from 0 to 360 degrees, and this will be corrected to the range of [-180, 180], which corresponds to lines: `project_ENGIE.py:105-107`\n",
    "\n",
    "The remainder of the data do not need modification aside from additional variable calculations (see `project_ENGIE.py` for more details) and the aforementioned timestamp conversions.\n",
    "\n",
    "## `PlantData` demonstration\n",
    "\n",
    "In `project_ENGIE.prepare()` there are two methods to return the data: by dataframe (`return_value=\"dataframes\"`) and by `PlantData` (`return_value=\"plantdata\"`), which are both demonstrated below.\n",
    "\n",
    "For the dataframe return selection, below it is also demonstrated how to load the dataframes into a `PlantData` object. A couple of things to notice about the creation of the the v3 `PlantData` object: \n",
    "- `metadata`: This field is what maps the OpenOA column convention to the user's column naming convention (`PlantData.update_column_names(to_original=True)` enables users to remap the data back to their original naming convention), in addition to a few other plant metadata objects.\n",
    "- `analysis_type`: This field controls how the data will be validated, if at all, based on the analysis requirements defined in `openoa.plant.ANALYSIS_REQUIREMENTS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209ed2a-d9b3-4301-8520-9efe0a77ab7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scada_df, meter_df, curtail_df, asset_df, reanalysis_dict = project_ENGIE.prepare(\n",
    "    path=\"data/la_haute_borne\",\n",
    "    return_value=\"dataframes\",\n",
    "    use_cleansed=False,\n",
    ")\n",
    "\n",
    "engie = PlantData(\n",
    "    analysis_type=None,  # No validation desired at this point in time\n",
    "    metadata=\"data/plant_meta.yml\",\n",
    "    scada=scada_df,\n",
    "    meter=meter_df,\n",
    "    curtail=curtail_df,\n",
    "    asset=asset_df,\n",
    "    reanalysis=reanalysis_dict,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52296eb8-cadb-4092-9c9d-7c04e9d37c51",
   "metadata": {},
   "source": [
    "Below is a summary of what the `PlantData` and `PlantMetaData` object specifications are, which are both new or effectively new as of version 3.0\n",
    "\n",
    "### `PlantData` documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f6a33-a957-4b7e-bf21-e81dec15997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PlantData.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d79db-1763-4783-8f33-8a9e7ea8bf6d",
   "metadata": {},
   "source": [
    "### `PlantMetaData` Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a09da8-a99c-49d7-bede-80d2aee98b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openoa.plant import PlantMetaData\n",
    "print(PlantMetaData.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fd21a-6bd0-4118-82de-0873abacd208",
   "metadata": {},
   "source": [
    "### `PlantData` validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec958a2-adda-4536-b3dd-451e9afd1808",
   "metadata": {},
   "source": [
    "Because `PlantData` is an attrs dataclass, it enables to support automatic conversion and validation of variables, which enables users to change analysis types as they work through their data. Below is a demonstration of what happens when we add an invalid analysis type (which fails to enable it), add the \"MonteCarloAEP\" analysis type (which passes the validation), and then further add in the \"all\" analysis type (which fails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf287a-6bb5-4843-9012-bb9fcc38c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    engie.analysis_type = \"MonteCarlo\"\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca786b1-0901-43d3-9a27-d70647f3d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "engie.analysis_type = \"MonteCarloAEP\"\n",
    "engie.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d1b4e-6521-4546-98f2-569bfbd99200",
   "metadata": {},
   "source": [
    "Notice that in the above cell, the data validates successfully for the MonteCarloAEP analysis, but below, when we append the `\"all\"` type, the validation fails. The below failure is because when `\"all\"` is input to `analysis_type`, it checks all of the data, and not just the analysis categories. In this case, there are no inputs to `tower` and `status`, so each of the checks will fail for all of the required columns by the metadata classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57b8a3-c761-405e-8cf5-af1b18eceb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "engie.analysis_type.append(\"all\")\n",
    "print(f\"The new analysis types now has all and MonteCarloAEP: {engie.analysis_type}\")\n",
    "try:\n",
    "    engie.validate()\n",
    "except ValueError as e: # Catch the error message so that the whole notebook can run\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17774b-fb28-45ad-b10d-fb581405dace",
   "metadata": {},
   "source": [
    "Below, the ENGIE data has been re-validated for a \"MonteCarloAEP\" analysis, so that it can be saved for later, and reloaded as the cleaned up version for easier importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6750a1-957b-40eb-853b-22185f4578cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "engie.analysis_type = \"MonteCarloAEP\"\n",
    "engie.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fbe78-ecd5-4f52-9176-c6fd5f218f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/cleansed\"\n",
    "engie.to_csv(save_path=data_path, with_openoa_col_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2e5d5-a0bb-4986-b6d1-e2d4df5f1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "engie_clean = PlantData(\n",
    "    metadata=f\"{data_path}/metadata.yml\",\n",
    "    scada=f\"{data_path}/scada.csv\",\n",
    "    meter=f\"{data_path}/meter.csv\",\n",
    "    curtail=f\"{data_path}/curtail.csv\",\n",
    "    asset=f\"{data_path}/asset.csv\",\n",
    "    reanalysis={\n",
    "        \"era5\": f\"{data_path}/reanalysis_era5.csv\",\n",
    "        \"merra2\": f\"{data_path}/reanalysis_merra2.csv\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cc428-ad96-4e1a-9c41-a4c22650e96a",
   "metadata": {},
   "source": [
    "### `PlantData` string and markdown representations\n",
    "\n",
    "Below, we can see a summary of all the data contained in the `PlantData` object, `engie_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453fed4-dbc4-4064-94af-bbd703cc91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(engie_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c816431-6826-4d4b-9ae2-827239539587",
   "metadata": {},
   "source": [
    "While this is really great, it is a bit more catered towards a terminal output, and so we provide a Jupyter-friendly Markdown representation as well, as can be seen below. Alternatively, `engie_clean.markdown()` can be called to ensure the data are correctly displayed in their markdown format.\n",
    "\n",
    "**NOTE**: the output displayed here that is viewable on the documentation site does not display the markdown correctly, so please take a look at the actual notebook on the GitHub or in your own Jupyter session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a8921-3fc7-4e8b-923d-d91114c6bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "engie_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80b434-6186-4750-bc3d-d17be845d05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "af55bea5a9de8d7c1e0d428b6c180049809c2d42782011e209ab7f34c0c55631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
