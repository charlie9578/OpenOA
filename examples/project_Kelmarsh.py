#################################################
# Data import script for Kelmarsh Project #
#################################################
"""
This is the import script for the example Cubico Kelmarsh project. Below
is a description of data quality for each data frame and an overview of the
steps taken to correct the raw data for use in the PRUF OA code.

1. SCADA dataframe
- 10-minute SCADA data for each of the turbines in the project
- Power, wind speed, wind direction, nacelle position, wind vane, temperature,
  blade pitch
- Corrects to UTC timezone when importing
- Removes some outliers and stuck sensor values

2. Meter data frame
- 10-minute performance data provided in energy units (kWh)
- Generated by adding artificial electrical loss and uncertaitny to SCADA data
- No need for timezone correction

3. Curtailment data frame
- 10-minute availability and curtailment data in kwh
- Generated by estimating availability and curtailment from SCADA data
- Below-normal production classified as curtailment if present at all turbines.
  Otherwise, classified as availability loss
- No need for timezone correction

4. Reanalysis products
- Import MERRA-2 and ERA5 1-hour reanalysis data
- Wind speed, wind direction, temperature, and density

"""
import os
from zipfile import ZipFile

import numpy as np
import pandas as pd

import operational_analysis.toolkits.timeseries as ts
import operational_analysis.toolkits.unit_conversion as un
import operational_analysis.toolkits.met_data_processing as met
from operational_analysis import logging, logged_method_call
from operational_analysis.types import PlantData
from operational_analysis.toolkits import filters


logger = logging.getLogger(__name__)


class Project_Kelmarsh(PlantData):
    """This class loads data for the ENGIE La Haute Borne site into a PlantData
    object"""

    def __init__(
        self,
        path="data/kelmarsh",
        name="Kelmarsh",
        engine="pandas",
        toolkit=["pruf_plant_analysis"],
    ):

        super(Project_Kelmarsh, self).__init__(path, name, engine, toolkit)

    def extract_data(self):
        """
        Extract data from zip files if they don't already exist.
        """
        if not os.path.exists(self._path):
            with ZipFile(self._path + ".zip") as zipfile:
                zipfile.extractall(self._path)

    def prepare(self):
        """
        Do all loading and preparation of the data for this plant.
        """

        # Extract data if necessary
        ## self.extract_data()

        # Set time frequencies of data in minutes
        self._meter_freq = "10T"  # Daily meter data
        self._curtail_freq = "10T"  # Daily curtailment data
        self._scada_freq = "10T"  # 10-min

        # Load meta data
        self._lat_lon = (52.40,-0.95)
        self._plant_capacity = 6*2.05  # MW
        self._num_turbines = 6
        self._turbine_capacity = 2.05  # MW


        ##############
        # ASSET DATA #
        ##############
        self._asset.load(self._path, "Kelmarsh_WT_static", "csv")
        self._asset.rename_columns(
            {
                "id": "Title",
                "latitude": "Latitude",
                "longitude": "Longitude",
                "rated_power_kw": "Rated power (kW)",
                "hub_height_m": "Hub Height (m)",
                "rotor_diameter_m": "Rotor Diameter (m)",
            }
        )

        # Assign type to turbine for all assets
        self._asset._asset["type"] = "turbine"

        # Drop renamed fields
        self._asset._asset.drop(
            [
                "Title",
                "Latitude",
                "Longitude",
                "Rated power (kW)",
                "Hub Height (m)",
                "Rotor Diameter (m)",
            ],
            axis=1,
            inplace=True,
        )


        ###################
        # SCADA DATA #
        ###################
        logger.info("Loading SCADA data")
        self._scada.load(self._path, "Kelmarsh_OpenOA_SCADA", "csv")  # Load Scada data
        logger.info("SCADA data loaded")

        logger.info("Timestamp QC and conversion to UTC")
        # Get 'time' field in datetime format. Local time zone information is
        # encoded, so convert to UTC

        self._scada.df["time"] = pd.to_datetime(
            self._scada.df["Timestamp"], utc=True,
            format="%Y-%m-%d %H:%M:%S"
        ).dt.tz_localize(None)

        # Remove duplicated timestamps and turbine id
        self._scada.df = self._scada.df.drop_duplicates(
            subset=["time", "Turbine"], keep="first"
        )

        # Set time as index
        self._scada.df.set_index("time", inplace=True, drop=False)

        logger.info("Correcting for out of range of temperature variables")
        # Handle extrema values for temperature. All other variables appear to
        # be reasonable.
        self._scada.df = self._scada.df[
            (self._scada.df["Nacelle ambient temperature (°C)"] >= -15.0) & (self._scada.df["Nacelle ambient temperature (°C)"] <= 50.0)
        ]

        # Add wind vane signal
        self._scada.df["Va_avg"] = self._scada.df["Nacelle position (°)"]-self._scada.df["Wind direction (°)"]

        logger.info("Flagging unresponsive sensors")
        # Due to data discretization, there appear to be a lot of repeating
        # values. But these filters seem to catch the obvious unresponsive
        # sensors.
        # for id in self._scada.df["Turbine"].unique():
        #     print(id)
        #     temp_flag = filters.unresponsive_flag(
        #         self._scada.df.loc[self._scada.df["Turbine"] == id, "Va_avg"], 3
        #     )
        #     self._scada.df.loc[
        #         (self._scada.df["Turbine"] == id) & (temp_flag),
        #         ["Blade angle (pitch position) A (°)", "Power (kW)", "Wind speed (m/s)", "Va_avg", "Nacelle ambient temperature (°C)", "Nacelle position (°)", "Wind direction (°)"],
        #     ] = np.nan
        #     temp_flag = filters.unresponsive_flag(
        #         self._scada.df.loc[self._scada.df["Turbine"] == id, "Nacelle ambient temperature (°C)"], 20
        #     )
        #     self._scada.df.loc[
        #         (self._scada.df["Turbine"] == id) & (temp_flag), "Nacelle ambient temperature (°C)"
        #     ] = np.nan

        # Put power in watts
        self._scada.df["Power_W"] = self._scada.df["Power (kW)"] * 1000

        # Convert pitch to range -180 to 180.
        self._scada.df["Blade angle (pitch position) A (°)"] = self._scada.df["Blade angle (pitch position) A (°)"] % 360
        self._scada.df.loc[self._scada.df["Blade angle (pitch position) A (°)"] > 180.0, "Blade angle (pitch position) A (°)"] = (
            self._scada.df.loc[self._scada.df["Blade angle (pitch position) A (°)"] > 180.0, "Blade angle (pitch position) A (°)"] - 360.0
        )

        # Calculate energy
        self._scada.df["energy_kwh"] = (
            un.convert_power_to_energy(self._scada.df["Power_W"], self._scada_freq) / 1000
        )

        logger.info("Converting field names to IEC 61400-25 standard")

        # Map to -25 standards
        scada_map = {
            "time": "time",
            "Turbine": "id",
            "Power_W": "wtur_W_avg",
            "Wind speed (m/s)": "wmet_wdspd_avg",
            "Wind direction (°)": "wmet_HorWdDir_avg",
            "Va_avg": "wmet_VaneDir_avg",
            "Nacelle position (°)": "wyaw_YwAng_avg",
            "Nacelle ambient temperature (°C)": "wmet_EnvTmp_avg",
            "Blade angle (pitch position) A (°)": "wrot_BlPthAngVal1_avg",
        }

        self._scada.df.rename(scada_map, axis="columns", inplace=True)

        # Remove the fields we are not yet interested in
        self._scada.df.drop(["Timestamp", "time", "Power (kW)"], axis=1, inplace=True)

        ##############
        # METER DATA #
        ##############
        
        # Currently the meter data is actually the sum of the turbine data
        # Once the fiscal meter data is fully uploaded this can be updated 
        # and will then account for electrical losses
        
        self._meter.load(self._path, "Kelmarsh_OpenOA_SCADA", "csv")  # Load Meter data

        # Create datetime field
        self._meter.df["time"] = pd.to_datetime(
            self._meter.df["Timestamp"], utc=True,
            format="%Y-%m-%d %H:%M:%S"
        ).dt.tz_localize(None)

        self._meter.df.set_index("time", inplace=True, drop=False)

        # Rename columns
        self._meter.df.rename(
            {
                "Energy Export (kWh)": "energy_kwh",
            },
            axis="columns", inplace=True
        )

        # Keep only the fields we need
        self._meter.df = self._meter.df[["energy_kwh"]]

        #####################################
        # Availability and Curtailment Data #
        #####################################
        self._curtail.load(self._path, "Kelmarsh_OpenOA_SCADA", "csv")  # Load Meter data

        # Create datetime field
        self._curtail.df["time"] = pd.to_datetime(
            self._curtail.df["Timestamp"], utc=True,
            format="%Y-%m-%d %H:%M:%S"
        ).dt.tz_localize(None)

        self._curtail.df.set_index("time", inplace=True, drop=False)

        # Rename columns
        self._curtail.df.rename(
            {
                "Lost Production to Downtime (kWh)":"availability_kwh",
                "Lost Production to Curtailment (Total) (kWh)":"curtailment_kwh",
            },
            axis="columns", inplace=True
        )

        # Keep only the fields we need
        self._curtail.df = self._curtail.df[["availability_kwh", "curtailment_kwh"]]


        ###################
        # REANALYSIS DATA #
        ###################

        # The data is downloaded through the PlanetOS API using the
        # toolkits.reanalysis_downloading module with the "planetos" option.
        # Follow the instructions to set up an API key, which needs to be placed
        # in the environment library (not the repo)

        logger.info("Loading reanalysis data")

        self._reanalysis.load(self._path,
                              self._name,
                              "planetos",
                              lat=self._lat_lon[0],
                              lon=self._lat_lon[1]
                              )

        # merra2
        self._reanalysis._product["merra2"].rename_columns({"time": "datetime"})

        self._reanalysis._product["merra2"].normalize_time_to_datetime("%Y-%m-%d %H:%M:%S")
        self._reanalysis._product["merra2"].df.set_index("time", inplace=True, drop=False)

        # Drop the fields we don't need
        self._reanalysis._product["merra2"].df.drop(["time","datetime"], axis=1, inplace=True)


        # era5
        self._reanalysis._product["era5"].rename_columns({"time": "datetime"})

        self._reanalysis._product["era5"].normalize_time_to_datetime("%Y-%m-%d %H:%M:%S")
        self._reanalysis._product["era5"].df.set_index("time", inplace=True, drop=False)

        # # Drop the fields we don't need
        self._reanalysis._product["era5"].df.drop(["time","datetime"], axis=1, inplace=True)

        
